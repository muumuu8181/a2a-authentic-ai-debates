# 月曜デモ用 詳細検証チェックリスト

**作成者**: Luna (Technical Reviewer)  
**日付**: 2025-08-01  
**目的**: A2A AI Debate System の完全性検証

---

## 🎯 検証カテゴリー

### 1. 基本機能検証 (Basic Functionality)

#### 1.1 AI会話の真正性
- [ ] **実際のAI応答確認**
  - テスト方法: `discussions/completed/`内のJSONファイルを確認
  - 確認項目: response_timeが1.5〜4秒の範囲内
  - 期待結果: 全ターンで実際のAPI呼び出しが行われている

- [ ] **ダミー応答の排除**
  - テスト方法: ai_agent_server.pyでGemini CLIが使用されているか確認
  - 確認項目: subprocess.runでgemini-cli.jsが呼ばれている
  - 期待結果: ハードコードされた応答が存在しない

#### 1.2 連続会話の実現
- [ ] **コンテキスト保持**
  - テスト方法: 6ターン以上の会話で前の発言への言及を確認
  - 確認項目: conversation_historyが正しく渡されている
  - 期待結果: 後半のターンで前半の議論への参照がある

- [ ] **議論の発展性**
  - テスト方法: 各ターンでの新しい論点の追加を確認
  - 確認項目: Argument Progression Indexが0.6以上
  - 期待結果: 単純な繰り返しではない議論の展開

### 2. 品質メトリクス検証 (Quality Metrics)

#### 2.1 Response Time Variance (RTV)
- [ ] **正常な分散の検出**
  - テスト方法: 本物のAI応答での分散計算
  - 確認項目: 分散値が0.3〜5.0の範囲内
  - 期待結果: Authenticity Score 100%

- [ ] **異常な一定性の検出**
  - テスト方法: 意図的に一定のタイミングでテスト
  - 確認項目: 分散値が0.3未満
  - 期待結果: Low authenticity警告（30%）

#### 2.2 Linguistic Fingerprint Analysis (LFA)
- [ ] **言語的多様性**
  - テスト方法: 各エージェントの言語パターン分析
  - 確認項目: 語彙、文構造、論証スタイルの違い
  - 期待結果: エージェント間で明確な差異

- [ ] **一貫性チェック**
  - テスト方法: 同一エージェントの複数応答を比較
  - 確認項目: 基本的なスタイルの一貫性
  - 期待結果: 80%以上の一貫性スコア

#### 2.3 エンゲージメント指標
- [ ] **Response Diversity Index**
  - テスト方法: 6ターン後のDiversityスコア確認
  - 確認項目: 初期100%から徐々に低下
  - 期待結果: 30%以下で警告

- [ ] **Debate Intensity Score**
  - テスト方法: 反論・新視点の出現頻度
  - 確認項目: challengesとnew_perspectivesのカウント
  - 期待結果: 各議論で3回以上の有意義な交換

### 3. セッション管理検証 (Session Management v2)

#### 3.1 チェックポイント機能
- [ ] **自動保存**
  - テスト方法: 各ターン後のチェックポイント確認
  - 確認項目: checkpoint_*.jsonファイルの生成
  - 期待結果: 全ターンで保存成功

- [ ] **データ完全性**
  - テスト方法: チェックポイントの内容検証
  - 確認項目: session_id, turn_number, conversation_context
  - 期待結果: 欠損データなし

#### 3.2 エラーハンドリング
- [ ] **リトライロジック**
  - テスト方法: ネットワークエラーのシミュレーション
  - 確認項目: 3回までの自動リトライ
  - 期待結果: エラー後の正常復旧

- [ ] **緊急チェックポイント**
  - テスト方法: エラー発生時の動作確認
  - 確認項目: エラー直前の状態保存
  - 期待結果: データロスなし

#### 3.3 状態管理
- [ ] **セッション状態遷移**
  - テスト方法: 各状態への遷移テスト
  - 確認項目: INITIALIZING → PENDING → ACTIVE → COMPLETED
  - 期待結果: 不正な遷移がない

- [ ] **並行セッション**
  - テスト方法: 2つのセッションを同時実行
  - 確認項目: セッション間の独立性
  - 期待結果: 相互干渉なし

### 4. 統合動作検証 (Integration)

#### 4.1 品質計算の統合
- [ ] **リアルタイム計算**
  - テスト方法: 各ターン後の品質スコア確認
  - 確認項目: quality_metrics.jsonの更新
  - 期待結果: 100ms以内に計算完了

- [ ] **ダッシュボード表示**
  - テスト方法: デモUIでのメトリクス表示
  - 確認項目: 全4カテゴリーのスコア表示
  - 期待結果: リアルタイム更新

#### 4.2 チェックポイントとの連携
- [ ] **品質スナップショット**
  - テスト方法: チェックポイント内の品質データ
  - 確認項目: 各時点での品質スコア保存
  - 期待結果: 完全な品質履歴

- [ ] **復旧時の品質継続**
  - テスト方法: チェックポイントからの再開
  - 確認項目: 品質メトリクスの継続性
  - 期待結果: スコアの断絶なし

### 5. パフォーマンス検証 (Performance)

#### 5.1 応答時間
- [ ] **AI応答時間**
  - 基準: 平均2-3秒、最大5秒
  - 測定方法: 各ターンのresponse_time記録

- [ ] **品質計算時間**
  - 基準: 100ms以内
  - 測定方法: calculate_overall_quality()の実行時間

- [ ] **チェックポイント保存時間**
  - 基準: 100ms以内
  - 測定方法: save_checkpoint()の実行時間

#### 5.2 リソース使用
- [ ] **メモリ使用量**
  - 基準: セッションあたり50MB以内
  - 測定方法: プロセスモニタリング

- [ ] **ストレージ使用量**
  - 基準: セッションあたり10MB以内
  - 測定方法: チェックポイントファイルサイズ合計

### 6. デモシナリオ検証 (Demo Scenarios)

#### 6.1 成功シナリオ
- [ ] **高品質な議論デモ**
  - シナリオ: 哲学的テーマでの深い議論
  - 確認項目: 全メトリクス80%以上
  - 期待結果: ボスが満足する質の高い会話

#### 6.2 検出シナリオ
- [ ] **「張りぼて会話」検出デモ**
  - シナリオ: わざと単調な応答を生成
  - 確認項目: Response Time Variance < 0.3
  - 期待結果: 自動的にLow authenticity警告

#### 6.3 復旧シナリオ
- [ ] **エラーからの復旧デモ**
  - シナリオ: 途中でエラーを発生させる
  - 確認項目: チェックポイントからの再開
  - 期待結果: シームレスな継続

## 📝 検証実施記録

### 実施者サイン欄
- [ ] Luna (Reviewer): _______________
- [ ] Oliver (Developer): _______________
- [ ] Boss (Final Approval): _______________

### 検証完了基準
- 全項目の95%以上が「✓」であること
- 重要項目（真正性、品質検出）は100%達成
- パフォーマンス基準を全て満たす

---

**次のステップ**: 
1. Oliverと一緒に各項目を検証
2. 問題があれば月曜朝一で修正
3. デモ前の最終確認

頑張りましょう！🚀